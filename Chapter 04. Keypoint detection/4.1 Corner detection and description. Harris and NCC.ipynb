{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Corner detection and description\n",
    "\n",
    "As you are learning during this course, images are highly valuable sources of information about the environment. That is why the use of cameras is becoming mainstream in a wider and wider set of real world applications such as robotics, autonomous cars, security, augmented and virtual reality, industrial inspection, etc. Images must be processed in order to obtain (compact) information about their content. For example, in the previous notebooks you learned how to detect edges in an image, which give us information, for example, about the contours of the objects in the scene. \n",
    "\n",
    "In general, there exist different types of elements that can be extracted from the images such as *corners*, *blobs*, *edges*, etc., each of them with different properties, and they are collectively called **image features**. In this chapter we will address one of the most common approaches for extracting information from images, namely **keypoint detection** (i.e. finding *interesting* pixels, or points, in the image).\n",
    "\n",
    "In this first notebook we will focus on detecting (and describing) *corners*: image points with high variation of intensity in two spatial directions. Concretely, we will cover corner detection using the well-known and widely used **Harris corner detector** (section 4.1.1). It is important to remark that the output of such detector are just the **positions** of the corners in the images (that's the purpose of a *detector*, actually). So, in order to be able to distinguish between different corners, we have to incorporate some kind of information about the **appearance** of the keypoint, which will be useful to match corresponding corners in different images. This is called the **description** of the corner. In this sense, we are going to explore how to use a patch around the keypoint as the descriptor, and employ the so-called **Normalized Cross-Correlation** (NCC) to find such matches (section 4.1.2). Finally, we will put these techniques to work together in section 4.1.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem context - Image stitching\n",
    "\n",
    "Image stitching or photo stitching is the process of combining multiple photographic images with overlapping fields of view to produce a segmented panorama or high-resolution image:\n",
    "\n",
    "<img src=\"./images/stitching.jpg\" width=\"600\">\n",
    "\n",
    "Feature detection is necessary to find correspondences between images. Robust correspondences are required in order to estimate the necessary transformation to align an image with the image it is being composited on. \n",
    "\n",
    "Our task in this notebook is to develop the first step in this process. **This lies in developing a robust point matching between overlapping images**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 20.0)\n",
    "images_path = './images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial data\n",
    "\n",
    "Let's start by **displaying our testing image** `car.jpeg` in the RGB color space. Tis is the image that we are going to use to familiarize with the concepts behind the Harris corner detector. Remember that by default OpenCV reads the image in BGR mode, so you'll have to convert it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read 'car.jpeg' image, convert to RGB and display it\n",
    "\n",
    "# Read image\n",
    "\n",
    "\n",
    "# Show it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1 Harris corner detector\n",
    "\n",
    "The **Harris detector** permits us to **detect corners** in an image. Recall that a corner is a point with high variation of intensity in 2 spatial directions:\n",
    "\n",
    "<img src=\"./images/corners.png\"  width=\"500\">$\\\\[5pt]$\n",
    "\n",
    "It has a number of appealing features:\n",
    "- **Simple and efficient implementation**.\n",
    "- **Robustness to noise**, since it considers the application of smoothing techniques. \n",
    "- It exhibits **invariance to**:\n",
    "  - **Rotation**: uses eigenvectors.\n",
    "  - **Brightness** (partially to contrast): uses derivatives.\n",
    "- Unfortunately **it's not invariant to scale**. \n",
    "\n",
    "The idea behind the Harris detector is to look for that high variations in the image. For that, let's consider a grayscale image $I$. Then, a window $w(x,y)$ with displacements $u$ in the x direction and $v$ in the y one is used to calculate variations of intensity:\n",
    "\n",
    "$$E(u,v) = \\sum_{x,y} w(x,y)[\\underbrace{I(x+u,y+v)}_\\text{Image shifted u,v}-\\underbrace{I(x,y)}_{Image}]^2$$\n",
    "\n",
    "where:\n",
    " - $w(x,y)$ is the weighting window centered at position $(x,y)$,\n",
    " - $I(x,y)$ is the pixel intensity at $(x,y)$, and\n",
    " - $I(x+u,y+v)$ is the pixel intensity at the moved window $(x+u,y+v)$\n",
    "\n",
    "The weighting $w(x,y)$ window typically is boolean (1 inside, 0 outside) or Gaussian, performing a Gaussian weighting. Thus, windows with large variations in intensity are good candidates for containing corners. This way, we have to maximize the previous equation, concretely the second term:\n",
    "\n",
    "$$\\sum_{x,y} [I(x+u,y+v)-I(x,y)]^2$$\n",
    "\n",
    "Using the Taylor expansion (first order Taylor approximation) it is written as:\n",
    "\n",
    "$$E(u,v)= \\sum_{x,y}[I(x,y) + uI_x + vI_v - I(x,y)]^2$$\n",
    "\n",
    "Notice that the first and the last terms cancel out. Expanding the equation:\n",
    "\n",
    "$$E(u,v) = \\sum_{x,y} u^2I_x^2 + v^2I_y^2 + 2uvI_xI_y$$\n",
    "\n",
    "Which can be expressed in matrix form as:\n",
    "\n",
    "$$\n",
    "E(u,v) \n",
    "\\approx \n",
    "\\begin{bmatrix}u & v\\end{bmatrix}\\underbrace{\\left(\\sum_{x,y} w(x,y) \n",
    "\\begin{bmatrix}\n",
    "I_x^2 & I_xI_y \\\\\n",
    "I_x I_y & I_y^2 \n",
    "\\end{bmatrix}\n",
    "\\right)}_{M}\n",
    "\\begin{bmatrix}\n",
    "u \\\\ v\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "E(u,v) \n",
    "\\approx \n",
    "\\begin{bmatrix}u & v\\end{bmatrix} M\n",
    "\\begin{bmatrix}\n",
    "u \\\\ v\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Recall that the matrix $\\mathbf{M}$ is computed for each pixel in the image, and it allows us to differentiate between pixels that are corners and pixels that are not. That matrix encompasses information about how the image intensity spatially changes in the surroundings or neighborhood of such pixel (the window). Therefore, it is not strange that image derivatives (e.g. Sobel) are involved in the building of $\\mathbf{M}$. We previously used them to detect edges, and a corner is a point where two edges make contact!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the magic! We can extract from $M$ the direction and strength of the intensity variation by **computing its eigenvectors $(\\mathbf{v}_1,\\mathbf{v}_2)$ and eigenvalues $(\\lambda_1, \\lambda_2)$**. This is done by considering a new reference frame at each image point, oriented along the direction of the highest average intensity derivative. For that, we diagonalize $M$ and consider a new eigenvector base $v_1$, $v_2$. That is:\n",
    "\n",
    "$$M=VDV^T,  \\ \\ \\ \\ D = \\begin{bmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{bmatrix}, \\ \\ \\ \\  V = \\begin{bmatrix} v_{1x} & v_{1y} \\\\ v_{2x} & v_{2y}\\end{bmatrix}$$\n",
    "\n",
    "so: \n",
    "\n",
    "$$\n",
    "E(u,v) \n",
    "\\approx \n",
    "\\begin{bmatrix}u & v\\end{bmatrix} M\n",
    "\\begin{bmatrix}\n",
    "u \\\\ v\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}u & v\\end{bmatrix} VDV^T\n",
    "\\begin{bmatrix}\n",
    "u \\\\ v\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The following image visually illustrates two eigenvector examples:\n",
    "\n",
    "<img src=\"./images/harris_eigen.PNG\" width=\"200\">$\\\\[5pt]$\n",
    "\n",
    "The largest eigenvalue, $\\lambda_1$, indicates how strong is the change in intensity along the principal direction of the change (given by $\\mathbf{v}_1$), while the smallest one, $ \\lambda_2$, does the same for its perpendicular direction $\\mathbf{v}_2$. This way, if both eigenvalues are large, it indicates that there is a large change in both directions and hence, the pixel is a good corner candidate. If only just one eigenvalue is large, then we are facing an edge and, finally, both eigenvalues are small in flat, textureless regions.\n",
    "\n",
    "<img src=\"./images/harris_eigenvalues_map.PNG\" width=\"350\">\n",
    "\n",
    "However, **computing eigenvalues and eigenvectors is a costly process**, and we need to do it for each pixel in the image! Thus, Harris proposed an alternative metric to get the same information. It is based on the definition of a scalar variable $R$ that indexes the same domain:\n",
    "\n",
    "$$R = \\underbrace{\\lambda_1 \\lambda_2}_\\text{Determinant} - k (\\underbrace{\\lambda_1+\\lambda_2}_\\text{Trace})^2$$\n",
    "\n",
    "and given that the trace and the determinant of $M$ and $D$ are the same, then:\n",
    "\n",
    "$$R = \\text{det}(\\mathbf{M}) - k (\\text{trace}(\\mathbf{M}))^2$$\n",
    "\n",
    "so **there is no need to compute the eigenvalues!** In these equations, $k$ is an empirically determined constant value in the range $(0.04-0.06)$.\n",
    "\n",
    "This $R$ value is interpreted as follows:\n",
    "- $R$ is large and positive at corners\n",
    "- $R$ is negative at edges\n",
    "- $R$ is small at flat regions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness to noise\n",
    "\n",
    "The window $w$ has another interesting role in the Harris detector: it smooths the computed derivatives, hence providing robustness against image noise. If the window is binary, then it performs average smoothing, and if it is Gaussian, it applies the smoothing technique with the same name. So, up to this point, we already know the tools that we are using: a border detector (e.g. Sobel) and a Gaussian filter/Average smoothing. Here we can see an example of the derivatives of the image and their smoothed versions:$\\\\[10pt]$\n",
    "\n",
    "<img src=\"./images/harris.png\"  width=\"600\">$\\\\[5pt]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"orange\">OpenCV pill</font>\n",
    "\n",
    "In practice, the Harris detector is already implemented in OpenCV with the method [`cv2.cornerHarris()`](https://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=cornerharris), which takes as inputs:\n",
    "\n",
    "- **src**: input grayscale image\n",
    "- **block_size**: neighborhood (window) size\n",
    "- **ksize**: size of the Sobel kernel (an odd value)\n",
    "- **k**: empiric constant (usually 0.04 - 0.06)\n",
    "\n",
    "and returns the $R$ value at each pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1a: R matrix</i></b></span>**\n",
    "\n",
    "Compute the $R$ matrix of the previous `car.jpeg` image using the Harris detector from OpenCV and display it. Take into account that the **Harris method** (and most keypoint detectors) **works with the grayscale version of the inputs images**. The color image will be used just for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1a\n",
    "# Convert to gray\n",
    "\n",
    "\n",
    "# Size of Sobel kernel\n",
    "\n",
    "\n",
    "# Empiric constant K\n",
    "\n",
    "\n",
    "# Size of Gaussian window (in openCV neighbor averaging)\n",
    "\n",
    "\n",
    "# Apply Harris\n",
    "\n",
    "\n",
    "# Show R image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plot the $R$ matrix, also called response map, you should see an image where borders are black pixels, corners are white ones, and flat surfaces are gray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b><i>Thinking about it (1)</i></b></font>\n",
    "\n",
    "**It is interesting** to play a bit with the input parameters. Then **answer the following questions**:\n",
    "\n",
    "- What happens if the size of $W$ is 1?\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- What happens if the size of $W$ is large? (*note that `cv2.cornerHarris()` uses neighborhood averaging instead of Gaussian filtering*)\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 0px 5px; color:blue\"><i>Your answer here!</i></p>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholding the response map $R$ \n",
    "\n",
    "The previous code computes the response map $R$ but, in the end, we need information about which image points contain corners, that is, a binary yes/no response. This is typically done by setting a threshold so pixels with a response higher than it are considered corners.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1b: Analyzing R</i></b></span>**\n",
    "\n",
    "In order to make an informed choice of that threshold, print the maximum and minimum values in the response map $R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1b\n",
    "#\n",
    "# Print max and min values of the R matrix (given)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are those values the same for different images or input parameters?  \n",
    "\n",
    "**No**, they are not. As they change, it is necessary to define the threshold relative to the maximum value in $R$ (in this way we skip normalization).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1c: Thresholding</i></b></span>**\n",
    "\n",
    "Use the OpenCV's [`cv2.threshold()`](https://docs.opencv.org/trunk/d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57) method to binarize the $R$ matrix using a threshold that you think is suitable. Then, display the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1c\n",
    "# Define the threshold wrt the maximum value, apply binarization to the image and display the result\n",
    "\n",
    "# Define the threshold wrt the maximum value\n",
    "\n",
    "\n",
    "# Apply binarization and show the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-maximum suppression\n",
    "\n",
    "The resulting binarized image shows white points where the R values are over the threshold and black points otherwise. However, around maximum values, there are many points that are close to each other. That is, we are getting multiple responses for the same corner.\n",
    "\n",
    "The general solution to this is to apply what is called **non-maximum suppression** to those clusters of points, keeping only a point for each of them (remember that this step was also part of the Canny edge detector!).$\\\\[10pt]$\n",
    "\n",
    "<img src=\"./images/nonmax.png\" width=\"600\">$\\\\[5pt]$\n",
    "\n",
    "To make this easier for you, we provide to you a `nonmaxsuppts()` function with an implementation of the non-maximum suppression step, which finds local maxima as corners within a window. This method returns two lists of numbers with the rows and columns, respectively, of the isolated corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method has been provided to you\n",
    "from scipy import signal\n",
    "def nonmaxsuppts(cim, radius, thresh):\n",
    "    \"\"\" Binarize and apply non-maximum suppresion.   \n",
    "    \n",
    "        Args:\n",
    "            cim: the harris 'R' image\n",
    "            radius: the aperture size of local maxima window\n",
    "            thresh: the threshold value for binarization\n",
    "                    \n",
    "        Returns: \n",
    "            r, c: two numpy vectors being the row (r) and the column (c) of each keypoint\n",
    "    \"\"\"   \n",
    "    \n",
    "    rows, cols = np.shape(cim)\n",
    "    sze = 2 * radius + 1\n",
    "    mx = signal.order_filter(cim, np.ones([sze, sze]), sze ** 2 - 1)\n",
    "    bordermask = np.zeros([rows, cols]);\n",
    "    bordermask[radius:(rows - radius), radius:(cols - radius)] = 1\n",
    "    cim = np.array(cim)\n",
    "    r, c = np.where((cim == mx) & (cim > thresh) & (bordermask == 1))\n",
    "    return r, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"orange\">OpenCV pill</font>\n",
    "\n",
    "In order to represent keypoints, OpenCV defines a set of functions for drawing objects related to feature detection. An example is [cv2.drawKeypoints](https://docs.opencv.org/3.4/d4/d5d/group__features2d__draw.html#gab958f8900dd10f14316521c149a60433), which given an image and a list of [cv2.KeyPoint](https://docs.opencv.org/3.4/d2/d29/classcv_1_1KeyPoint.html) objects, it draws them on the output image (third input argument).\n",
    "\n",
    "Note that this implies the transformation of `r` and `c` to a `cv2.KeyPoint` list. \n",
    "\n",
    "*Tip: Have a look at: [rows and columns to KeyPoint list ideas](https://www.programcreek.com/python/example/77058/cv2.KeyPoint)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1d: Non-maxima suppression</i></b></span>**\n",
    "\n",
    "Apply non-maxima suppression to the previous $R$ image and draw the detected keypoints on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignment 1d\n",
    "# Apply non-max suppression to R and display the remaining keypoints on the image\n",
    "# Write your code here!\n",
    "\n",
    "# Define the radius of the keypoint neighborhood to be considered\n",
    "\n",
    "\n",
    "# Apply non maxima supression\n",
    "\n",
    "\n",
    "# Convert coordinates to cv2.KeyPoint list\n",
    "\n",
    "\n",
    "# Draw keypoints\n",
    "\n",
    "\n",
    "# Show image with corners\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2 Keypoint matching through NCC\n",
    "\n",
    "One important thing to understand is that after extracting the keypoints, you only obtain information about their position. While this information might be enough for some applications (e.g. counting corners), it does not say much about the keypoints themselves. For example, in applications where you need to match keypoints, that is, identify the correspondence of one keypoint in an image with those in the other one, to know the pixel location is not enough. This reveals the need to **describe** such keypoints, and match them according to these descriptions. The simplest descriptor is to consider the intensity of the pixels surrounding a keypoint, i.e. to extract **a patch of the image** with the keypoint at its center. But, how to match keypoints according to these patches?\n",
    "\n",
    "At this point is where the **Normalized Cross-Correlation (NCC)** method could help. Computing the NCC is similar to applying a convolution, i.e. a sliding window (the image patch, template, or kernel) is applied to an image. Unlike in the convolution case, NCC does not flip the kernel/patch. $\\\\[5pt]$\n",
    "\n",
    "<img src=\"./images/ncc_convolution.jpg\" width=\"400\">$\\\\[5pt]$\n",
    "\n",
    "Moreover, the *Normalized* in NCC means that the results are normalized in order to be invariant to changes in brightness and contrast of the image $f$ and patch $h$.\n",
    "\n",
    "$$NCC(x,y) = \\sum_{i,j}\\hat f (x+i,y+j) \\hat h(i,j)$$\n",
    "\n",
    "where $\\hat f$ and $\\hat h$ have zero mean and contrast one.\n",
    "\n",
    "The intuition behind this method is that, after applying NCC, **the most similar points to the patch in the image take higher NCC values** (closer to 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"orange\">OpenCV pill</font>\n",
    "\n",
    "OpenCV implements the function [`cv2.matchTemplate()`](https://docs.opencv.org/2.4/modules/imgproc/doc/object_detection.html?highlight=matchtemplate), which takes as first argument an `image`, and applies NCC with the `template` or patch provided as second argument (as set by its third argument: `cv2.TM_CCORR_NORMED`). That is, the template slides over every pixel in the image and its NCC response is computed, giving a score indicating how much the template looks like the image at that pixel. \n",
    "\n",
    "This code will help you to understand how NCC works.   \n",
    "- **Review it and try to explain what is done here** (you can modify `p_index` and `w_temp` parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This snippet has been provided to you. Play with 'p_index' and 'w_temp' values\n",
    "\n",
    "# Index of the focusing point\n",
    "p_index = 26\n",
    "\n",
    "# NCC window aperture\n",
    "w_temp = 20\n",
    "\n",
    "# Get the NCC window\n",
    "p_r, p_c = r[p_index], c[p_index]\n",
    "template = image[p_r-w_temp:p_r+w_temp+1,p_c-w_temp:p_c+w_temp+1]\n",
    "\n",
    "# Calculate the NCC image (note the cv2.TM_CCORR_NORMED parameter!)\n",
    "ncc = cv2.matchTemplate(image, template, cv2.TM_CCORR_NORMED)\n",
    "\n",
    "# Search the maximum in NCC\n",
    "p_x, p_y = p_c, p_r\n",
    "[max_y, max_x] = np.where(ncc == np.amax(ncc))\n",
    "\n",
    "# Draw a rectangle in matching points\n",
    "image_draw = np.copy(image)\n",
    "ncc_draw = np.copy(ncc)\n",
    "\n",
    "cv2.rectangle(image_draw,(int(p_x-w_temp),int(p_y-w_temp)),(int(p_x+w_temp+1),int(p_y+w_temp+1)),(0,255,0),3)\n",
    "cv2.rectangle(ncc_draw,(int(max_x-w_temp),int(max_y-w_temp)),(int(max_x+w_temp+1),int(max_y+w_temp+1)),0,3)\n",
    "\n",
    "# Show the original image\n",
    "plt.subplot(121)\n",
    "plt.imshow(image_draw)\n",
    "plt.title('Original image')\n",
    "\n",
    "# Show the NCC image\n",
    "plt.subplot(122)\n",
    "plt.imshow(ncc_draw, cmap='gray')\n",
    "plt.title('NCC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the patch with the green rectangle in the left image has a maximum NCC value at the black rectangle in the right image. As you can imagine, since the template is extracted from the same image, the NCC value will be 1 at the same exact position (the template and the image at that point are exactly the same!). But this is just an example to illustrate how all this works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b><i>Thinking about it (2)</i></b></font>\n",
    "\n",
    "Now, **answer the following questions**:\n",
    "\n",
    "- Describe what variables `p_index` and `w_temp` do.\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- What does the NCC image/response represent?\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- What would happen if `template` was taken from a similar (but not the same) image?\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.3 Time to work! Find matches between images with (some) overlapping\n",
    "\n",
    "It is time to apply what have been learned during this notebook. Coming back to image stitching, **you are asked** to compute matches between two images with overlapping areas. The images are `park_l.jpeg` (left) and `park_r.jpeg` (right).\n",
    "\n",
    "**For that, you should use the Harris corner detector and the NCC descriptor**. Also, you will have to draw the keypoints and the matches using [`cv2.drawMatches()`](https://docs.opencv.org/3.4/d4/d5d/group__features2d__draw.html#ga7421b3941617d7267e3f2311582f49e1), which takes two images, two lists of `cv2.KeyPoint` objects and a list of [`cv2.DMatch`](https://docs.opencv.org/3.4/d4/de0/classcv_1_1DMatch.html) objects as input. To create a `cv2.DMatch` object it is used `cv2.DMatch(idx_l, idx_r,1)`, being `idx_l` and `idx_r` the indices of the matching keypoints (in the left and right lists of keypoints, respectively).\n",
    "\n",
    "*Tip: [Drawing functions documentation](https://docs.opencv.org/master/d4/d5d/group__features2d__draw.html)* \n",
    "\n",
    "Results should look like this (output of [`cv2.drawMatches()`](https://docs.opencv.org/3.4/d4/d5d/group__features2d__draw.html#ga7421b3941617d7267e3f2311582f49e1)):\n",
    "\n",
    "<img src=\"./images/index.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2a: First, detect keypoints with Harris</i></b></span>**\n",
    "\n",
    "- Use the Harris corner detector to get the keypoints of both grayscale images (do not forget to apply non-max suppression to the resulting $R$ image!)\n",
    "- Convert them to two lists of `cv2.KeyPoint` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 2a\n",
    "# Detect Harris keypoints in the images and convert them to a 'cv2.Keypoint' list\n",
    "\n",
    "# Size of Sobel kernel\n",
    "\n",
    "\n",
    "# Size of Gaussian window (in openCV neighbor averaging)\n",
    "\n",
    "\n",
    "# Empiric constant K\n",
    "\n",
    "\n",
    "# Read images\n",
    "\n",
    "\n",
    "# Get gray images\n",
    "\n",
    "\n",
    "# Compute corners\n",
    "\n",
    "\n",
    "# For non-max-supp\n",
    "\n",
    "\n",
    "# Apply non maximum supression\n",
    "\n",
    "\n",
    "# Convert coordinates to cv2.KeyPoint list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2b: Avoiding issues</i></b></span>**\n",
    "\n",
    "Now, we will address an intermediate step to avoid future issues. As we will need to extract a template around the keypoints and it might happen that those can be detected too close to the border (e.g. a keypoint found at (0,0)), we will add a *padding* to the image using the OpenCV's method\n",
    "[`cv2.copyMakeBorder()`](https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html?highlight=copymakeborder#void%20copyMakeBorder(InputArray%20src,%20OutputArray%20dst,%20int%20top,%20int%20bottom,%20int%20left,%20int%20right,%20int%20borderType,%20const%20Scalar&%20value)) (`cv2.BORDER_REFLECT` is the best padding option in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 2b\n",
    "# Add padding to the images\n",
    "\n",
    "\n",
    "# Aperture size of the NCC window\n",
    "\n",
    "\n",
    "# Apply border reflect padding to images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2c: Matching</i></b></span>**\n",
    "\n",
    "And finally:\n",
    "- For every $i$-th keypoint in the left image:\n",
    "    1. Take a template around that point.\n",
    "    - Compute the NCC value using that template with **the whole right image**, as in the provided example.\n",
    "    - Since we want to retrieve true matches, find **the maximum NCC value** at the positions of the keypoints in the right image.\n",
    "    - If the maximum NCC value (for a $j$ keypoint in the right image) is greater than a certain threshold (e.g. 0.95, but tune it for robustness), create a `cv2.DMatch` object with the indexes $(i,j)$ of the keypoints and add it to a list of matches.\n",
    "- Once all matches are defined, use the `cv2.drawMatches()` method to display the resulting color image with the matches.\n",
    "\n",
    "*Tip: When displaying the resulting image, it is preferable to make a copy of the original color image and use it as the output image for the `cv2.drawMatches()` method. This way it keeps the same size as the original one.*\n",
    "\n",
    "Be careful with the number of keypoints detected, as **it could take a time to process all of them!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assignment 2c\n",
    "# Match the keypoints in both images by using NCC, get a 'cv2.DMatch' list and display the matches\n",
    "\n",
    "\n",
    "# Initialize matches list\n",
    "\n",
    "\n",
    "# For each keypoint in left image\n",
    "\n",
    "    \n",
    "    # Get the template (note the w_temp offset because of the padding in previous step)\n",
    "    \n",
    "    \n",
    "    # Compute NCC of the left keypoint in right image\n",
    "    \n",
    "\n",
    "    # Find max value in NCC (only at the right keypoints)\n",
    "    \n",
    "    \n",
    "    # If match is good\n",
    "    \n",
    "        # Include in match list\n",
    "        \n",
    "\n",
    "# Cast matches list to cv2.DMatch list\n",
    "\n",
    "\n",
    "# Draw matches\n",
    "\n",
    "\n",
    "# And show them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You have been able to find keypoints in two images and match them! You would probably have found that **not all matches are correct**, and that is normal. Take into account that matching keypoints based on their appearance around a small patch is not really easy for computers (not even for humans!). Of course, there are other methods to describe the keypoints that are more robust, as we will see next.\n",
    "\n",
    "In deployed applications, this *outliers* (i.e. wrong associations) problem is addressed by implementing certain filters that incorporate more information about the scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This was a hard work, but finally you made it! In this notebook you have learned to:\n",
    "\n",
    "- detect corners using the Harris detector,\n",
    "- match corners using image patches and NCC, and\n",
    "- develop the first steps for an image stitching system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 375.51666600000004,
   "position": {
    "height": "397.35px",
    "left": "1091.58px",
    "right": "20px",
    "top": "164px",
    "width": "523.667px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
